{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask B - Classify UNT / TIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>clean_tweets</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ask nativ american</td>\n",
       "      <td>[ask, nativ, american]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>home drunk manga trump</td>\n",
       "      <td>[home, drunk, manga, trump]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>amazon investig chines employe sell intern dat...</td>\n",
       "      <td>[amazon, investig, chines, employe, sell, inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>someon retaken piec shit volcano</td>\n",
       "      <td>[someon, retaken, piec, shit, volcano]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>obama want liber amp illeg move red state</td>\n",
       "      <td>[obama, want, liber, amp, illeg, move, red, st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "\n",
       "  subtask_b subtask_c                                       clean_tweets  \\\n",
       "0       UNT       NaN                                 ask nativ american   \n",
       "1       TIN       IND                             home drunk manga trump   \n",
       "2       NaN       NaN  amazon investig chines employe sell intern dat...   \n",
       "3       UNT       NaN                   someon retaken piec shit volcano   \n",
       "4       NaN       NaN          obama want liber amp illeg move red state   \n",
       "\n",
       "                                              tokens  \n",
       "0                             [ask, nativ, american]  \n",
       "1                        [home, drunk, manga, trump]  \n",
       "2  [amazon, investig, chines, employe, sell, inte...  \n",
       "3             [someon, retaken, piec, shit, volcano]  \n",
       "4  [obama, want, liber, amp, illeg, move, red, st...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import utils.preprocessing\n",
    "from utils.test_model import *\n",
    "from utils.utils import make_submission\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('data/raw/offenseval-training-v1.tsv', sep='\\t')\n",
    "utils.preprocessing.clean(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweets</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15923</td>\n",
       "      <td>#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...</td>\n",
       "      <td>whose wherestheserv dumpsit declasfisa democra...</td>\n",
       "      <td>[whose, wherestheserv, dumpsit, declasfisa, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60133</td>\n",
       "      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n",
       "      <td>nopasaran uniti demo oppos far right london an...</td>\n",
       "      <td>[nopasaran, uniti, demo, oppos, far, right, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83681</td>\n",
       "      <td>. . . What the fuck did he do this time?</td>\n",
       "      <td>fuck time</td>\n",
       "      <td>[fuck, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65507</td>\n",
       "      <td>@USER Do you get the feeling he is kissing @US...</td>\n",
       "      <td>feel kiss behind humili later</td>\n",
       "      <td>[feel, kiss, behind, humili, later]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12588</td>\n",
       "      <td>@USER Nigga ware da hits at</td>\n",
       "      <td>nigga ware hit</td>\n",
       "      <td>[nigga, ware, hit]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  \\\n",
       "0  15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...   \n",
       "1  60133  #NoPasaran: Unity demo to oppose the far-right...   \n",
       "2  83681           . . . What the fuck did he do this time?   \n",
       "3  65507  @USER Do you get the feeling he is kissing @US...   \n",
       "4  12588                        @USER Nigga ware da hits at   \n",
       "\n",
       "                                        clean_tweets  \\\n",
       "0  whose wherestheserv dumpsit declasfisa democra...   \n",
       "1  nopasaran uniti demo oppos far right london an...   \n",
       "2                                          fuck time   \n",
       "3                      feel kiss behind humili later   \n",
       "4                                     nigga ware hit   \n",
       "\n",
       "                                              tokens  \n",
       "0  [whose, wherestheserv, dumpsit, declasfisa, de...  \n",
       "1  [nopasaran, uniti, demo, oppos, far, right, lo...  \n",
       "2                                       [fuck, time]  \n",
       "3                [feel, kiss, behind, humili, later]  \n",
       "4                                 [nigga, ware, hit]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = df[\"clean_tweets\"]\n",
    "df_test = pd.read_csv('data/test/task_b/testset-taskb.tsv', sep='\\t')\n",
    "utils.preprocessing.clean(df_test)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing to remomve NaNs in labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score ALL TIN 0.4529\n",
      "Nan proportion (removed): 66.77%\n",
      "1.0    3876\n",
      "0.0     524\n",
      "Name: subtask_b, dtype: int64\n",
      "Category split in cleaned dataset:\n",
      "\tTIN: 88.09%\n",
      "\tUNT: 11.91%\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1,3), stop_words='english', max_features=38269)\n",
    "\n",
    "y_train = df[\"subtask_b\"].map({\"UNT\":0, \"TIN\":1})\n",
    "\n",
    "all_tin = pd.Series(1, index=np.arange(len(y_train)))\n",
    "y_fill = y_train.copy()\n",
    "y_fill[np.isnan(y_fill)] = 0\n",
    "print('F1 score ALL TIN', round(metrics.f1_score(y_fill,all_tin),4))\n",
    "\n",
    "# remove rows with NaN as label in y_train and X then build X_train\n",
    "(nan_idx,) = np.where(~np.isnan(y_train))\n",
    "print('Nan proportion (removed): {}%'.format(100*round(1-len(nan_idx) / y_train.shape[0], 4)))\n",
    "\n",
    "# clean from NaNs ans reset index to prevent errors in testing\n",
    "y_train = y_train.dropna().reset_index()['subtask_b']\n",
    "X_new = X.iloc[nan_idx].reset_index()['clean_tweets']\n",
    "\n",
    "X_train = vec.fit_transform(X_new)\n",
    "X_test = vec.transform(df_test[\"clean_tweets\"])\n",
    "\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print('Category split in cleaned dataset:')\n",
    "print('\\tTIN: {}%'.format(100* round(y_train.value_counts()[1] / y_train.count(), 4)))\n",
    "print('\\tUNT: {}%'.format(100* round(y_train.value_counts()[0] / y_train.count(), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=2, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l2', random_state=None,\n",
      "          solver='sag', tol=0.0001, verbose=0, warm_start=False)\n",
      "=================================================================\n",
      "=========================== Training ============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================== Evaluating ===========================\n",
      "Accuracy:\t0.9711\n",
      "========================== Validating ===========================\n",
      "Mean F1 score (3-fold CV):\t0.5815\n",
      "Mean Accuracy (3-fold CV):\t0.8277\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=2, dual=False, class_weight='balanced', solver='sag')\n",
    "print(logreg)\n",
    "print('=================================================================')\n",
    "\n",
    "# Train Model\n",
    "print('=========================== Training ============================')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Model\n",
    "print('========================== Evaluating ===========================')\n",
    "print('Accuracy:\\t{}'.format(round(logreg.score(X_train, y_train),4)))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "print('========================== Validating ===========================')\n",
    "(f1_logreg, acc_logreg) = test_single_model(logreg, X_new, y_train, vec, n_splits=3, random_state=1)\n",
    "print('Mean F1 score (3-fold CV):\\t{}'.format(round(f1_logreg,4)))\n",
    "print('Mean Accuracy (3-fold CV):\\t{}'.format(round(acc_logreg,4)))\n",
    "\n",
    "# Predict on test set and write submission\n",
    "y_pred_proba_logreg = logreg.predict_proba(X_test)\n",
    "y_pred_logreg = np.argmax(y_pred_proba_logreg,1)\n",
    "make_submission(y_pred_logreg, {0:\"UNT\", 1:\"TIN\"}, df_test, \"submissions/taskb_logreg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RidgeClassifier(alpha=0.01, class_weight=None, copy_X=True,\n",
      "        fit_intercept=True, max_iter=None, normalize=False,\n",
      "        random_state=None, solver='sag', tol=0.001)\n",
      "=================================================================\n",
      "=========================== Training ============================\n",
      "========================== Evaluating ===========================\n",
      "Accuracy:\t0.9891\n",
      "========================== Validating ===========================\n",
      "Mean F1 score (3-fold CV):\t0.5231\n",
      "Mean Accuracy (3-fold CV):\t0.8211\n"
     ]
    }
   ],
   "source": [
    "ridge = RidgeClassifier(alpha=1e-2, solver='sag')\n",
    "print(ridge)\n",
    "print('=================================================================')\n",
    "\n",
    "# Train Model\n",
    "print('=========================== Training ============================')\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Model\n",
    "print('========================== Evaluating ===========================')\n",
    "print('Accuracy:\\t{}'.format(round(ridge.score(X_train, y_train),4)))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "print('========================== Validating ===========================')\n",
    "(f1_ridge, acc_ridge) = test_single_model(ridge, X_new, y_train, vec, n_splits=3, random_state=1)\n",
    "print('Mean F1 score (3-fold CV):\\t{}'.format(round(f1_ridge,4)))\n",
    "print('Mean Accuracy (3-fold CV):\\t{}'.format(round(acc_ridge,4)))\n",
    "\n",
    "# Predict on test set and write submission\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "make_submission(y_pred_ridge, {0:\"UNT\", 1:\"TIN\"}, df_test, \"submissions/taskb_ridge.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "=================================================================\n",
      "=========================== Training ============================\n",
      "========================== Evaluating ===========================\n",
      "Accuracy:\t0.8809\n",
      "========================== Validating ===========================\n",
      "Mean F1 score (3-fold CV):\t0.4683\n",
      "Mean Accuracy (3-fold CV):\t0.8809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(decision_function_shape='ovo', probability=True, gamma='auto')\n",
    "print(svc)\n",
    "print('=================================================================')\n",
    "\n",
    "# Train Model\n",
    "print('=========================== Training ============================')\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Model\n",
    "print('========================== Evaluating ===========================')\n",
    "print('Accuracy:\\t{}'.format(round(svc.score(X_train, y_train),4)))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "print('========================== Validating ===========================')\n",
    "(f1_svc, acc_svc) = test_single_model(svc, X_new, y_train, vec, n_splits=3, random_state=1)\n",
    "print('Mean F1 score (3-fold CV):\\t{}'.format(round(f1_svc,4)))\n",
    "print('Mean Accuracy (3-fold CV):\\t{}'.format(round(acc_svc,4)))\n",
    "\n",
    "# Predict on test set and write submission\n",
    "y_pred_proba_svc = svc.predict_proba(X_test)\n",
    "y_pred_svc = np.argmax(y_pred_proba_svc,1)\n",
    "make_submission(y_pred_svc, {0:\"UNT\", 1:\"TIN\"}, df_test, \"submissions/taskb_svc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data “augmentation“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "word_vec = CountVectorizer(ngram_range=(1,3), stop_words='english',analyzer='word', max_features=32500)\n",
    "train_word_features = word_vec.fit_transform(X_new)\n",
    "test_word_features = word_vec.fit_transform(df_test[\"clean_tweets\"])\n",
    "\n",
    "char_vec = CountVectorizer(ngram_range=(1,3), stop_words='english',analyzer='char', max_features=32500)\n",
    "train_char_features = char_vec.fit_transform(X_new)\n",
    "test_char_features = char_vec.fit_transform(df_test[\"clean_tweets\"])\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=100, max_features=1000,\n",
      "            max_leaf_nodes=50, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=3,\n",
      "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "=================================================================\n",
      "=========================== Training ============================\n",
      "========================== Evaluating ===========================\n",
      "Accuracy:\t0.7882\n",
      "========================== Validating ===========================\n",
      "Mean F1 score (3-fold CV):\t0.5697\n",
      "Mean Accuracy (3-fold CV):\t0.7664\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(bootstrap=True, class_weight='balanced', criterion='gini',\n",
    "            max_depth=100, max_features=1000, max_leaf_nodes=50,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=3, min_samples_split=10,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "print(rfc)\n",
    "print('=================================================================')\n",
    "\n",
    "# Train Model\n",
    "print('=========================== Training ============================')\n",
    "rfc.fit(train_features, y_train)\n",
    "\n",
    "# Evaluate Model\n",
    "print('========================== Evaluating ===========================')\n",
    "print('Accuracy:\\t{}'.format(round(rfc.score(train_features, y_train),4)))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "print('========================== Validating ===========================')\n",
    "(f1_rfc, acc_rfc) = test_single_model(rfc, X_new, y_train, [word_vec, char_vec], \n",
    "                                      n_splits=3, random_state=1)\n",
    "print('Mean F1 score (3-fold CV):\\t{}'.format(round(f1_rfc,4)))\n",
    "print('Mean Accuracy (3-fold CV):\\t{}'.format(round(acc_rfc,4)))\n",
    "\n",
    "# Predict on test set and write submission\n",
    "y_pred_proba_rfc = rfc.predict_proba(X_test)\n",
    "y_pred_rfc = np.argmax(y_pred_proba_rfc,1)\n",
    "make_submission(y_pred_rfc, {0:\"UNT\", 1:\"TIN\"}, df_test, \"submissions/taskb_rfc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=100, max_features=1000,\n",
      "            max_leaf_nodes=50, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=3,\n",
      "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "=================================================================\n",
      "=========================== Training ============================\n",
      "========================== Evaluating ===========================\n",
      "Accuracy:\t0.7302\n",
      "========================== Validating ===========================\n",
      "Mean F1 score (3-fold CV):\t0.5609\n",
      "Mean Accuracy (3-fold CV):\t0.717\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(bootstrap=True, class_weight='balanced', criterion='gini',\n",
    "            max_depth=100, max_features=1000, max_leaf_nodes=50,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=3, min_samples_split=10,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "print(rfc)\n",
    "print('=================================================================')\n",
    "\n",
    "# Train Model\n",
    "print('=========================== Training ============================')\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Model\n",
    "print('========================== Evaluating ===========================')\n",
    "print('Accuracy:\\t{}'.format(round(rfc.score(X_train, y_train),4)))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "print('========================== Validating ===========================')\n",
    "(f1_rfc, acc_rfc) = test_single_model(rfc, X_new, y_train, vec, \n",
    "                                      n_splits=3, random_state=1)\n",
    "print('Mean F1 score (3-fold CV):\\t{}'.format(round(f1_rfc,4)))\n",
    "print('Mean Accuracy (3-fold CV):\\t{}'.format(round(acc_rfc,4)))\n",
    "\n",
    "# Predict on test set and write submission\n",
    "y_pred_proba_rfc = rfc.predict_proba(X_test)\n",
    "y_pred_rfc = np.argmax(y_pred_proba_rfc,1)\n",
    "make_submission(y_pred_rfc, {0:\"UNT\", 1:\"TIN\"}, df_test, \"submissions/taskb_rfc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble : RF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 score (3-fold CV):\t0.5933\n",
      "Mean Accuracy (3-fold CV):\t0.8193\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba_ens = (y_pred_proba_logreg + y_pred_proba_rfc) / 2.0\n",
    "y_pred_ens = np.argmax(y_pred_proba_ens, 1)\n",
    "make_submission(y_pred_ens, {0:\"UNT\", 1:\"TIN\"}, df_test, \"submissions/taskb_ens_rf_lr.csv\")\n",
    "\n",
    "f1_lr_rf, acc_lf_rf = test_voting_model([logreg, rfc], X_new, y_train, {'sklearn':vec}, n_splits=3, random_state=1)\n",
    "print('Mean F1 score (3-fold CV):\\t{}'.format(round(f1_lr_rf,4)))\n",
    "print('Mean Accuracy (3-fold CV):\\t{}'.format(round(acc_lf_rf,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Manip for Deep Learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from utils.keras_utils import f1_loss\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Embedding, Bidirectional, LSTM, GlobalMaxPool1D, Dropout, Dense\n",
    "from keras.preprocessing import text\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Embedding\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import graphviz\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=15000)\n",
    "tokenizer.fit_on_texts(X_new)\n",
    "\n",
    "X_test = df_test[\"clean_tweets\"]\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_new)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "y_cat = to_categorical(y_train)\n",
    "\n",
    "X_tr = sequence.pad_sequences(list_tokenized_train, maxlen=100)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 16)           240000    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 96, 32)            2592      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 24, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                12304     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 255,050\n",
      "Trainable params: 255,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "cnn = Sequential()\n",
    "cnn.add(Embedding(15000, 16, input_length=100))\n",
    "cnn.add(Conv1D(32, 5, activation='relu'))\n",
    "cnn.add(MaxPooling1D(4))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dropout(rate=0.25))\n",
    "cnn.add(Dense(units=16, activation='relu'))\n",
    "cnn.add(Dropout(rate=0.15))\n",
    "cnn.add(Dense(units=8, activation='relu'))\n",
    "cnn.add(Dense(units=2, activation='softmax'))\n",
    "cnn.compile(loss=f1_loss, optimizer='adam', metrics=['accuracy'])\n",
    "print(cnn.summary())\n",
    "print('=================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "print('=========================== Training ============================')\n",
    "cnn.fit(X_tr, y_cat, epochs=10,  batch_size=64, verbose=0)\n",
    "\n",
    "# Evaluate Model\n",
    "print('========================== Evaluating ===========================')\n",
    "evaluation_cnn = cnn.evaluate(X_tr, y_cat, batch_size=64, verbose=0)\n",
    "print('{}:\\t{}'.format(cnn.metrics_names[0], round(evaluation_cnn[0],4)))\n",
    "print('{}:\\t{}'.format(cnn.metrics_names[1], round(evaluation_cnn[1],4)))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "print('========================== Validating ===========================')\n",
    "(f1_cnn, acc_cnn) = test_single_model(cnn, X_new, y_train, tokenizer, epochs=10,\n",
    "                                      n_splits=3, random_state=1, verbose=0)\n",
    "print('Mean F1 score (3-fold CV):\\t{}'.format(round(f1_cnn,4)))\n",
    "print('Mean Accuracy (3-fold CV):\\t{}'.format(round(acc_cnn,4)))\n",
    "\n",
    "# Predict on test set and write submission\n",
    "y_pred_proba_cnn = cnn.predict(X_te)\n",
    "y_pred_cnn = np.argmax(y_pred_proba_cnn, axis=1)\n",
    "make_submission(y_pred_cnn, {0:\"UNT\", 1:\"TIN\"}, df_test, \"submissions/taskb_cnn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 16)           240000    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 32)           4224      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 244,506\n",
      "Trainable params: 244,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "=================================================================\n",
      "=========================== Training ============================\n",
      "========================== Evaluating ===========================\n",
      "loss:\t0.129\n",
      "acc:\t0.9505\n",
      "========================== Validating ===========================\n",
      "Mean F1 score (3-fold CV):\t0.6728\n",
      "Mean Accuracy (3-fold CV):\t0.8637\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "lstm = Sequential()\n",
    "lstm.add(Embedding(15000, 16, input_length=100))\n",
    "lstm.add(Bidirectional(LSTM(16, return_sequences=True)))\n",
    "lstm.add(GlobalMaxPool1D())\n",
    "lstm.add(Dropout(0.15))\n",
    "lstm.add(Dense(8, activation=\"relu\"))\n",
    "lstm.add(Dense(2, activation=\"softmax\"))\n",
    "lstm.compile(loss=f1_loss, optimizer='adam', metrics=['accuracy'])\n",
    "print(lstm.summary())\n",
    "print('=================================================================')\n",
    "\n",
    "# Train Model\n",
    "print('=========================== Training ============================')\n",
    "lstm.fit(X_tr, y_cat, epochs=10,  batch_size=64, verbose=0)\n",
    "\n",
    "# Evaluate Model\n",
    "print('========================== Evaluating ===========================')\n",
    "evaluation_lstm = lstm.evaluate(X_tr, y_cat, batch_size=64, verbose=0)\n",
    "print('{}:\\t{}'.format(lstm.metrics_names[0], round(evaluation_lstm[0],4)))\n",
    "print('{}:\\t{}'.format(lstm.metrics_names[1], round(evaluation_lstm[1],4)))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "print('========================== Validating ===========================')\n",
    "(f1_lstm, acc_lstm) = test_single_model(lstm, X_new, y_train, tokenizer, epochs=10,\n",
    "                                        n_splits=3, random_state=1, verbose=0)\n",
    "print('Mean F1 score (3-fold CV):\\t{}'.format(round(f1_lstm,4)))\n",
    "print('Mean Accuracy (3-fold CV):\\t{}'.format(round(acc_lstm,4)))\n",
    "\n",
    "# Predict on test set and write submission\n",
    "y_pred_proba_lstm = lstm.predict(X_te)\n",
    "y_pred_lstm = np.argmax(y_pred_proba_lstm, axis=1)\n",
    "make_submission(y_pred_lstm, {0:\"UNT\", 1:\"TIN\"}, df_test, \"submissions/taskb_lstm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM + ConvLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 100, 8)            120000    \n",
      "_________________________________________________________________\n",
      "bidirectional_18 (Bidirectio (None, 100, 8)            416       \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 98, 8)             200       \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_13 (Glo (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 120,794\n",
      "Trainable params: 120,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "blc = Sequential()\n",
    "blc.add(Embedding(15000, 8, input_length=100, trainable=True))\n",
    "blc.add(Bidirectional(LSTM(4, return_sequences=True, dropout=0.15, recurrent_dropout=0.15)))\n",
    "blc.add(Conv1D(8, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform'))\n",
    "blc.add(GlobalMaxPool1D())\n",
    "blc.add(Dropout(0.5))\n",
    "blc.add(Dense(16, activation='relu'))\n",
    "blc.add(Dense(2, activation='softmax'))\n",
    "blc.compile(loss=f1_loss, optimizer='adam', metrics=['accuracy'])\n",
    "print(blc.summary())\n",
    "print('=================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blc = Sequential()\n",
    "blc.add(Embedding(15000, 16, input_length=100, trainable=True))\n",
    "blc.add(Bidirectional(LSTM(16, return_sequences=True, dropout=0.15, recurrent_dropout=0.15)))\n",
    "blc.add(Conv1D(16, kernel_size=5))\n",
    "blc.add(Dropout(0.25))\n",
    "blc.add(Conv1D(8, kernel_size=5))\n",
    "blc.add(Dropout(0.25))\n",
    "blc.add(Conv1D(8, kernel_size=5))\n",
    "blc.add(Flatten())\n",
    "blc.add(Dropout(0.5))\n",
    "blc.add(Dense(8, activation='relu'))\n",
    "blc.add(Dense(2, activation='softmax'))\n",
    "blc.compile(loss=f1_loss, optimizer='adam', metrics=['accuracy'])\n",
    "print(blc.summary())\n",
    "print('=================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================== Training ============================\n",
      "========================== Evaluating ===========================\n",
      "loss:\t0.3864\n",
      "acc:\t0.8845\n",
      "========================== Validating ===========================\n",
      "Mean F1 score (3-fold CV):\t0.6554\n",
      "Mean Accuracy (3-fold CV):\t0.845\n"
     ]
    }
   ],
   "source": [
    "# Train Model\n",
    "print('=========================== Training ============================')\n",
    "blc.fit(X_tr, y_cat, epochs=4,  batch_size=64, verbose=0)\n",
    "\n",
    "# Evaluate Model\n",
    "print('========================== Evaluating ===========================')\n",
    "evaluation_blc = blc.evaluate(X_tr, y_cat, batch_size=64, verbose=0)\n",
    "print('{}:\\t{}'.format(blc.metrics_names[0], round(evaluation_blc[0],4)))\n",
    "print('{}:\\t{}'.format(blc.metrics_names[1], round(evaluation_blc[1],4)))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "print('========================== Validating ===========================')\n",
    "(f1_blc, acc_blc) = test_single_model(blc, X_new, y_train, tokenizer, epochs=4,\n",
    "                                      n_splits=3, random_state=1, verbose=0)\n",
    "print('Mean F1 score (3-fold CV):\\t{}'.format(round(f1_blc,4)))\n",
    "print('Mean Accuracy (3-fold CV):\\t{}'.format(round(acc_blc,4)))\n",
    "\n",
    "# Predict on test set and write submission\n",
    "y_pred_proba_blc = blc.predict(X_te)\n",
    "y_pred_blc = np.argmax(y_pred_proba_blc, axis=1)\n",
    "make_submission(y_pred_blc, {0:\"UNT\", 1:\"TIN\"}, df_test, \"submissions/taskb_blc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi Supervised Learning ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
