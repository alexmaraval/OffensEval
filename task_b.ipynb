{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask B - Classify UNT / TIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Alexandre/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>subtask_a</th>\n",
       "      <th>subtask_b</th>\n",
       "      <th>subtask_c</th>\n",
       "      <th>clean_tweets</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86426</td>\n",
       "      <td>@USER She should ask a few native Americans wh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ask nativ american</td>\n",
       "      <td>[ask, nativ, american]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90194</td>\n",
       "      <td>@USER @USER Go home you’re drunk!!! @USER #MAG...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>TIN</td>\n",
       "      <td>IND</td>\n",
       "      <td>home drunk manga trump</td>\n",
       "      <td>[home, drunk, manga, trump]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16820</td>\n",
       "      <td>Amazon is investigating Chinese employees who ...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>amazon investig chines employe sell intern dat...</td>\n",
       "      <td>[amazon, investig, chines, employe, sell, inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62688</td>\n",
       "      <td>@USER Someone should'veTaken\" this piece of sh...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>UNT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>someon retaken piec shit volcano</td>\n",
       "      <td>[someon, retaken, piec, shit, volcano]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43605</td>\n",
       "      <td>@USER @USER Obama wanted liberals &amp;amp; illega...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>obama want liber amp illeg move red state</td>\n",
       "      <td>[obama, want, liber, amp, illeg, move, red, st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet subtask_a  \\\n",
       "0  86426  @USER She should ask a few native Americans wh...       OFF   \n",
       "1  90194  @USER @USER Go home you’re drunk!!! @USER #MAG...       OFF   \n",
       "2  16820  Amazon is investigating Chinese employees who ...       NOT   \n",
       "3  62688  @USER Someone should'veTaken\" this piece of sh...       OFF   \n",
       "4  43605  @USER @USER Obama wanted liberals &amp; illega...       NOT   \n",
       "\n",
       "  subtask_b subtask_c                                       clean_tweets  \\\n",
       "0       UNT       NaN                                 ask nativ american   \n",
       "1       TIN       IND                             home drunk manga trump   \n",
       "2       NaN       NaN  amazon investig chines employe sell intern dat...   \n",
       "3       UNT       NaN                   someon retaken piec shit volcano   \n",
       "4       NaN       NaN          obama want liber amp illeg move red state   \n",
       "\n",
       "                                              tokens  \n",
       "0                             [ask, nativ, american]  \n",
       "1                        [home, drunk, manga, trump]  \n",
       "2  [amazon, investig, chines, employe, sell, inte...  \n",
       "3             [someon, retaken, piec, shit, volcano]  \n",
       "4  [obama, want, liber, amp, illeg, move, red, st...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import utils.preprocessing\n",
    "from utils.test_model import *\n",
    "from utils.utils import make_submission\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/raw/offenseval-training-v1.tsv', sep='\\t')\n",
    "utils.preprocessing.clean(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweets</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15923</td>\n",
       "      <td>#WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...</td>\n",
       "      <td>whose wherestheserv dumpsit declasfisa democra...</td>\n",
       "      <td>[whose, wherestheserv, dumpsit, declasfisa, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60133</td>\n",
       "      <td>#NoPasaran: Unity demo to oppose the far-right...</td>\n",
       "      <td>nopasaran uniti demo oppos far right london an...</td>\n",
       "      <td>[nopasaran, uniti, demo, oppos, far, right, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83681</td>\n",
       "      <td>. . . What the fuck did he do this time?</td>\n",
       "      <td>fuck time</td>\n",
       "      <td>[fuck, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65507</td>\n",
       "      <td>@USER Do you get the feeling he is kissing @US...</td>\n",
       "      <td>feel kiss behind humili later</td>\n",
       "      <td>[feel, kiss, behind, humili, later]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12588</td>\n",
       "      <td>@USER Nigga ware da hits at</td>\n",
       "      <td>nigga ware hit</td>\n",
       "      <td>[nigga, ware, hit]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  \\\n",
       "0  15923  #WhoIsQ #WheresTheServer #DumpNike #DECLASFISA...   \n",
       "1  60133  #NoPasaran: Unity demo to oppose the far-right...   \n",
       "2  83681           . . . What the fuck did he do this time?   \n",
       "3  65507  @USER Do you get the feeling he is kissing @US...   \n",
       "4  12588                        @USER Nigga ware da hits at   \n",
       "\n",
       "                                        clean_tweets  \\\n",
       "0  whose wherestheserv dumpsit declasfisa democra...   \n",
       "1  nopasaran uniti demo oppos far right london an...   \n",
       "2                                          fuck time   \n",
       "3                      feel kiss behind humili later   \n",
       "4                                     nigga ware hit   \n",
       "\n",
       "                                              tokens  \n",
       "0  [whose, wherestheserv, dumpsit, declasfisa, de...  \n",
       "1  [nopasaran, uniti, demo, oppos, far, right, lo...  \n",
       "2                                       [fuck, time]  \n",
       "3                [feel, kiss, behind, humili, later]  \n",
       "4                                 [nigga, ware, hit]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = df[\"clean_tweets\"]\n",
    "df_test = pd.read_csv('data/test/task_b/testset-taskb.tsv', sep='\\t')\n",
    "utils.preprocessing.clean(df_test)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing to remomve NaNs in labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nan proportion (removed): 66.77%\n",
      "1.0    3876\n",
      "0.0     524\n",
      "Name: subtask_b, dtype: int64\n",
      "Category split in cleaned dataset:\n",
      "\tTIN: 88.09%\n",
      "\tUNT: 11.91%\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1,3), stop_words='english', max_features=38269)\n",
    "\n",
    "y_train = df[\"subtask_b\"].map({\"UNT\":0, \"TIN\":1})\n",
    "\n",
    "# remove rows with NaN as label in y_train and X then build X_train\n",
    "(nan_idx,) = np.where(~np.isnan(y_train))\n",
    "print('Nan proportion (removed): {}%'.format(100*round(1-len(nan_idx) / y_train.shape[0], 4)))\n",
    "\n",
    "# clean from NaNs ans reset index to prevent errors in testing\n",
    "y_train = y_train.dropna().reset_index()['subtask_b']\n",
    "X_new = X.iloc[nan_idx].reset_index()['clean_tweets']\n",
    "\n",
    "X_train = vec.fit_transform(X_new)\n",
    "X_test = vec.transform(df_test[\"clean_tweets\"])\n",
    "\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print('Category split in cleaned dataset:')\n",
    "print('\\tTIN: {}%'.format(100* round(y_train.value_counts()[1] / y_train.count(), 4)))\n",
    "print('\\tUNT: {}%'.format(100* round(y_train.value_counts()[0] / y_train.count(), 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=4, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n",
      "=================================================================\n",
      "=========================== Training ============================\n",
      "========================== Evaluating ===========================\n",
      "Accuracy:\t98.4773\n",
      "========================== Validating ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 score (3-fold CV):\t50.9031%\n",
      "Mean Accuracy (3-fold CV):\t87.0681%\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=4, dual=True)\n",
    "print(logreg)\n",
    "print('=================================================================')\n",
    "\n",
    "# Train Model\n",
    "print('=========================== Training ============================')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Model\n",
    "print('========================== Evaluating ===========================')\n",
    "print('Accuracy:\\t{}'.format(round(100*logreg.score(X_train, y_train),4)))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "print('========================== Validating ===========================')\n",
    "(f1_logreg, acc_logreg) = test_single_model(logreg, X_new, y_train, vec, n_splits=3, random_state=1)\n",
    "print('Mean F1 score (3-fold CV):\\t{}%'.format(round(100*f1_logreg,4)))\n",
    "print('Mean Accuracy (3-fold CV):\\t{}%'.format(round(100*acc_logreg,4)))\n",
    "\n",
    "# Predict on test set and write submission\n",
    "y_pred_proba_logreg = logreg.predict_proba(X_test)\n",
    "y_pred_logreg = np.argmax(y_pred_proba_logreg,1)\n",
    "make_submission(y_pred_logreg, {\"UNT\":0, \"TIN\":1}, df_test, \"submissions/taskb_logreg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=4, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovo', degree=3, gamma='auto_deprecated',\n",
      "  kernel='sigmoid', max_iter=-1, probability=True, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "=================================================================\n",
      "=========================== Training ============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================== Evaluating ===========================\n",
      "Accuracy:\t88.0909\n",
      "========================== Validating ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean F1 score (3-fold CV):\t46.8332%\n",
      "Mean Accuracy (3-fold CV):\t88.0908%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC(C=4, kernel='sigmoid', shrinking=True, decision_function_shape='ovo', probability=True)\n",
    "print(svc)\n",
    "print('=================================================================')\n",
    "\n",
    "# Train Model\n",
    "print('=========================== Training ============================')\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Model\n",
    "print('========================== Evaluating ===========================')\n",
    "print('Accuracy:\\t{}'.format(round(100*svc.score(X_train, y_train),4)))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "print('========================== Validating ===========================')\n",
    "(f1_svc, acc_svc) = test_single_model(svc, X_new, y_train, vec, n_splits=3, random_state=1)\n",
    "print('Mean F1 score (3-fold CV):\\t{}%'.format(round(100*f1_svc,4)))\n",
    "print('Mean Accuracy (3-fold CV):\\t{}%'.format(round(100*acc_svc,4)))\n",
    "\n",
    "# Predict on test set and write submission\n",
    "y_pred_proba_svc = svc.predict_proba(X_test)\n",
    "y_pred_svc = np.argmax(y_pred_proba_svc,1)\n",
    "make_submission(y_pred_svc, {\"UNT\":0, \"TIN\":1}, df_test, \"submissions/taskb_svc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### data “augmentation“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "word_vec = CountVectorizer(ngram_range=(1,3), stop_words='english',analyzer='word', max_features=32500)\n",
    "train_word_features = word_vec.fit_transform(X_new)\n",
    "test_word_features = word_vec.fit_transform(df_test[\"clean_tweets\"])\n",
    "\n",
    "char_vec = CountVectorizer(ngram_range=(1,3), stop_words='english',analyzer='char', max_features=32500)\n",
    "train_char_features = char_vec.fit_transform(X_new)\n",
    "test_char_features = char_vec.fit_transform(df_test[\"clean_tweets\"])\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features])\n",
    "test_features = hstack([test_char_features, test_word_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=100, max_features=1000,\n",
      "            max_leaf_nodes=50, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=3,\n",
      "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "=================================================================\n",
      "=========================== Training ============================\n",
      "========================== Evaluating ===========================\n",
      "Accuracy:\t79.1364\n",
      "========================== Validating ===========================\n",
      "Mean F1 score (3-fold CV):\t57.0667%\n",
      "Mean Accuracy (3-fold CV):\t76.7043%\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(bootstrap=True, class_weight='balanced', criterion='gini',\n",
    "            max_depth=100, max_features=1000, max_leaf_nodes=50,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=3, min_samples_split=10,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=None, verbose=0,\n",
    "            warm_start=False)\n",
    "\n",
    "print(rfc)\n",
    "print('=================================================================')\n",
    "\n",
    "# Train Model\n",
    "print('=========================== Training ============================')\n",
    "rfc.fit(train_features, y_train)\n",
    "\n",
    "# Evaluate Model\n",
    "print('========================== Evaluating ===========================')\n",
    "print('Accuracy:\\t{}'.format(round(100*rfc.score(train_features, y_train),4)))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "print('========================== Validating ===========================')\n",
    "(f1_rfc, acc_rfc) = test_single_model(rfc, X_new, y_train, [word_vec, char_vec], \n",
    "                                      n_splits=3, random_state=1)\n",
    "print('Mean F1 score (3-fold CV):\\t{}%'.format(round(100*f1_rfc,4)))\n",
    "print('Mean Accuracy (3-fold CV):\\t{}%'.format(round(100*acc_rfc,4)))\n",
    "\n",
    "# Predict on test set and write submission\n",
    "y_pred_proba_rfc = rfc.predict_proba(X_test)\n",
    "y_pred_rfc = np.argmax(y_pred_proba_rfc,1)\n",
    "make_submission(y_pred_rfc, {\"UNT\":0, \"TIN\":1}, df_test, \"submissions/taskb_rfc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble : RF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba_ens = (y_pred_proba_logreg + y_pred_proba_rfc) / 2.0\n",
    "y_pred_ens = np.argmax(y_pred_proba_ens, 1)\n",
    "make_submission(y_pred_ens, {0:\"UNT\", 1:\"TIN\"}, df_test, \"submissions/taskb_ens_rf_lr.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Manip for Deep Learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from utils.keras_utils import f1_loss\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Embedding, Bidirectional, LSTM, GlobalMaxPool1D, Dropout, Dense\n",
    "from keras.preprocessing import text\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Embedding\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=15000)\n",
    "tokenizer.fit_on_texts(X_new)\n",
    "\n",
    "X_test = df_test[\"clean_tweets\"]\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(X_new)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "y_cat = to_categorical(y_train)\n",
    "\n",
    "X_tr = sequence.pad_sequences(list_tokenized_train, maxlen=100)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 16)           240000    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 97, 16)            1040      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 24, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 16)                6160      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 247,354\n",
      "Trainable params: 247,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "=================================================================\n",
      "=========================== Training ============================\n",
      "========================== Evaluating ===========================\n",
      "4400/4400 [==============================] - 0s 106us/step\n",
      "loss:\t9.7928\n",
      "acc:\t96.5%\n",
      "========================== Validating ===========================\n",
      "Mean F1 score (3-fold CV):\t68.8621%\n",
      "Mean Accuracy (3-fold CV):\t87.6155%\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "cnn = Sequential()\n",
    "cnn.add(Embedding(15000, 16, input_length=100))\n",
    "cnn.add(Conv1D(16, 4, activation='relu'))\n",
    "cnn.add(MaxPooling1D(4))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dropout(rate=0.25))\n",
    "cnn.add(Dense(units=16, activation='relu'))\n",
    "cnn.add(Dropout(rate=0.15))\n",
    "cnn.add(Dense(units=8, activation='relu'))\n",
    "cnn.add(Dense(units=2, activation='softmax'))\n",
    "cnn.compile(loss=f1_loss, optimizer='adam', metrics=['accuracy'])\n",
    "print(cnn.summary())\n",
    "print('=================================================================')\n",
    "\n",
    "# Train Model\n",
    "print('=========================== Training ============================')\n",
    "cnn.fit(X_tr, y_cat, epochs=20,  batch_size=64, verbose=False)\n",
    "\n",
    "# Evaluate Model\n",
    "print('========================== Evaluating ===========================')\n",
    "evaluation_cnn = cnn.evaluate(X_tr, y_cat, batch_size=64, verbose=1)\n",
    "print('{}:\\t{}'.format(cnn.metrics_names[0], round(100*evaluation_cnn[0],4)))\n",
    "print('{}:\\t{}%'.format(cnn.metrics_names[1], round(100*evaluation_cnn[1],4)))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "print('========================== Validating ===========================')\n",
    "(f1_cnn, acc_cnn) = test_single_model(cnn, X_new, y_train, tokenizer, epochs=10,\n",
    "                                      n_splits=3, random_state=1, verbose=False)\n",
    "print('Mean F1 score (3-fold CV):\\t{}%'.format(round(100*f1_cnn,4)))\n",
    "print('Mean Accuracy (3-fold CV):\\t{}%'.format(round(100*acc_cnn,4)))\n",
    "\n",
    "# Predict on test set and write submission\n",
    "y_pred_proba_cnn = cnn.predict(X_te)\n",
    "y_pred_cnn = np.argmax(y_pred_proba_cnn, axis=1)\n",
    "make_submission(y_pred_cnn, {\"UNT\":0, \"TIN\":1}, df_test, \"submissions/taskb_cnn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 32)           480000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 32)           6272      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 486,554\n",
      "Trainable params: 486,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "=================================================================\n",
      "=========================== Training ============================\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "========================== Evaluating ===========================\n",
      "4400/4400 [==============================] - 2s 422us/step\n",
      "loss:\t16.4371\n",
      "acc:\t94.2045%\n",
      "========================== Validating ===========================\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "lstm = Sequential()\n",
    "lstm.add(Embedding(15000, 32, input_length=100))\n",
    "lstm.add(Bidirectional(LSTM(16, return_sequences=True)))\n",
    "lstm.add(GlobalMaxPool1D())\n",
    "lstm.add(Dropout(0.15))\n",
    "lstm.add(Dense(8, activation=\"relu\"))\n",
    "lstm.add(Dense(2, activation=\"softmax\"))\n",
    "lstm.compile(loss=f1_loss, optimizer='adam', metrics=['accuracy'])\n",
    "print(lstm.summary())\n",
    "print('=================================================================')\n",
    "\n",
    "# Train Model\n",
    "print('=========================== Training ============================')\n",
    "lstm.fit(X_tr, y_cat, epochs=10,  batch_size=64, verbose=False)\n",
    "\n",
    "# Evaluate Model\n",
    "print('========================== Evaluating ===========================')\n",
    "evaluation_lstm = lstm.evaluate(X_tr, y_cat, batch_size=64, verbose=1)\n",
    "print('{}:\\t{}'.format(lstm.metrics_names[0], round(100*evaluation_lstm[0],4)))\n",
    "print('{}:\\t{}%'.format(lstm.metrics_names[1], round(100*evaluation_lstm[1],4)))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "print('========================== Validating ===========================')\n",
    "test_single_model(lstm, X_new, y_train, tokenizer, n_splits=3, random_state=1, verbose=False)\n",
    "(f1_lstm, acc_lstm) = test_single_model(lstm, X_new, y_train, tokenizer, \n",
    "                                        n_splits=3, random_state=1, verbose=False)\n",
    "print('Mean F1 score (3-fold CV):\\t{}%'.format(round(100*f1_lstm,4)))\n",
    "print('Mean Accuracy (3-fold CV):\\t{}%'.format(round(100*acc_lstm,4)))\n",
    "\n",
    "# Predict on test set and write submission\n",
    "y_pred_proba_lstm = lstm.predict(X_te)\n",
    "y_pred_lstm = np.argmax(y_pred_proba_lstm, axis=1)\n",
    "make_submission(y_pred_lstm, {\"UNT\":0, \"TIN\":1}, df_test, \"submissions/taskb_lstm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM + ConvLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blc = Sequential()\n",
    "blc.add(Embedding(15000, 16, input_length=100, trainable=True))\n",
    "blc.add(Bidirectional(LSTM(16, return_sequences=True, dropout=0.15, recurrent_dropout=0.15)))\n",
    "blc.add(Conv1D(16, kernel_size=4, padding='valid', kernel_initializer='glorot_uniform'))\n",
    "blc.add(GlobalMaxPool1D())\n",
    "blc.add(Dropout(0.15))\n",
    "blc.add(Dense(8, activation='relu'))\n",
    "blc.add(Dense(2, activation='softmax'))\n",
    "blc.compile(loss=f1_loss, optimizer='adam', metrics=['accuracy'])\n",
    "print(blc.summary())\n",
    "print('=================================================================')\n",
    "\n",
    "# Train Model\n",
    "print('=========================== Training ============================')\n",
    "blc.fit(X_tr, y_cat, epochs=4,  batch_size=64, verbose=False)\n",
    "\n",
    "# Evaluate Model\n",
    "print('========================== Evaluating ===========================')\n",
    "evaluation_blc = blc.evaluate(X_tr, y_cat, batch_size=64, verbose=1)\n",
    "print('{}:\\t{}'.format(blc.metrics_names[0], round(100*evaluation_blc[0],4)))\n",
    "print('{}:\\t{}%'.format(blc.metrics_names[1], round(100*evaluation_blc[1],4)))\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "print('========================== Validating ===========================')\n",
    "test_single_model(blc, X_new, y_train, tokenizer, n_splits=3, random_state=1, verbose=False)\n",
    "(f1_blc, acc_blc) = test_single_model(blc, X_new, y_train, tokenizer, \n",
    "                                      n_splits=3, random_state=1, verbose=False)\n",
    "print('Mean F1 score (3-fold CV):\\t{}%'.format(round(100*f1_blc,4)))\n",
    "print('Mean Accuracy (3-fold CV):\\t{}%'.format(round(100*acc_blc,4)))\n",
    "\n",
    "# Predict on test set and write submission\n",
    "y_pred_proba_blc = blc.predict(X_te)\n",
    "y_pred_blc = np.argmax(y_pred_proba_blc, axis=1)\n",
    "make_submission(y_pred_blc, {\"UNT\":0, \"TIN\":1}, df_test, \"submissions/taskb_blc.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi Supervised Learning ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
